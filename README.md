# 🧠 Project Optics – Real-Time ASL Recognition Model 🤟 by ThunderGuyk, and ProGamerTejaji, and Shyam and other friends lmao

**Project Optics** is a deep learning-powered application that recognizes American Sign Language (ASL) gestures in real-time using your webcam. Trained on a custom dataset and engineered with love, math, and way too many `.pt` files, this model bridges silence with speech using the language of light and code.

> “Where words fail, fingers speak.”

---

## 🚀 Features

- 🔮 **Live Prediction:** Captures and interprets ASL signs in real time using your webcam.
- 🎯 **Custom Trained Model:** Leveraging PyTorch for efficient and accurate gesture recognition.
- 🧰 **Modular Code:** Easy to understand, extend, and customize.
- ⚡ **Fast & Responsive:** Optimized inference for low latency predictions.
- 👁️ **Visualization:** Optional bounding box, label display, and performance stats overlay.

---

## 🛠️ How It Works

1. **Data Collection:** ASL images/videos were gathered, labeled, and preprocessed.
2. **Model Training:** A CNN (or your architecture of choice) was trained using PyTorch.
3. **Live Inference:** OpenCV captures live feed, and the trained model predicts each frame's sign.

---

## 📦 Installation

Clone this repo:

```bash
git clone https://github.com/your-username/project-optics.git
cd project-optics
